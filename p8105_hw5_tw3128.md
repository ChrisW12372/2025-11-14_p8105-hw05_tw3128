p8105_hw5_tw3128
================
Chris
2025-11-14

# Problem One

## fixed group size

``` r
one_birthday_trial = function(n){
  birthdays = sample(1:365, size = n, replace = TRUE)
  repeated_bday <- length(unique(birthdays)) < n
  repeated_bday
}

one_birthday_trial(300)
```

    ## [1] TRUE

## monte carlo simulation

``` r
estimate_bdays_prob <- function(n, B = 10000) {
  if(!is.numeric(n)){
    stop("n must be numeric.")
  }
  mean(
    replicate(B, one_birthday_trial(n))
    )
}
```

``` r
df_prob <- 
  expand_grid(n = 2:50) |>
  rowwise() |>
  mutate(prob = estimate_bdays_prob(n)) |>
  ungroup()

df_prob
```

    ## # A tibble: 49 × 2
    ##        n   prob
    ##    <int>  <dbl>
    ##  1     2 0.0032
    ##  2     3 0.0086
    ##  3     4 0.0159
    ##  4     5 0.0279
    ##  5     6 0.0362
    ##  6     7 0.0583
    ##  7     8 0.0816
    ##  8     9 0.0968
    ##  9    10 0.117 
    ## 10    11 0.136 
    ## # ℹ 39 more rows

## plotting

``` r
df_prob |>
  ggplot(aes(x = n, y = prob)) +
  geom_line(alpha = 0.8, color = "#5a6acd") +
  labs(
    title = "Estimated Probability of Birthday Problem (2-50)",
    x = "n (group size)",
    y = "Probability"
  )
```

![](p8105_hw5_tw3128_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

- Overall Trend
  - The probability of two people sharing the same birthday increased
    monotonically as n increases from 2 to 50.  
  - Larger groups generate more possible pairs, which naturally raises
    the chance of a shared birthday.
- Birthday Paradox
  - When `n = 23`, the probability is already larger than `0.5`, which
    feels counterintuitive because 23 is far smaller than 365.  
  - The reason is that the number of pairs is
    $\frac{n \times (n - 1)}{2}$, which can be larger than 365 even when
    n is small.

# Problem Two

## one sample t-test for $\mu = 0$

``` r
simulate_tt = function(mu, n = 30, sigma = 5, B = 5000){
  replicate(
    B, {
      x <- rnorm(n = n, mean = mu, sd = sigma)
      tt <- t.test(x, mu = 0, )
      df_tt_tidy <- tidy(tt)
      df_estimations <- 
        tibble(mu_hat = df_tt_tidy$estimate, p_value = df_tt_tidy$p.value)
    },
    simplify = FALSE) |>
    
    bind_rows() |>
    mutate(id = row_number()) |>
    relocate(id)
}

sim0 <- simulate_tt(0)
```

## when $\mu$ equals other values

``` r
mu_values <- 1:6

df_power <-
  map_dfr(
    mu_values,
    function(true_mu){
      sim <- simulate_tt(true_mu)
      power <- mean(sim$p_value < 0.05)
      
      tibble(
        true_mu ,
        power 
      )
    }
  )

df_power |>
  ggplot(aes(x = true_mu, y = power)) + 
  geom_line(color = "#6a5acd") + geom_point(size = 2, color = "steelblue") +
  labs(
    title = "The Relationship Between Power and Effect Size",
    x = expression("real " *  mu),
    y = "power"
  )
```

![](p8105_hw5_tw3128_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

- Overall Trend
  - The powers of simple t tests increase monotonically when $\mu$
    increases from 1 to 6.
- Segmented Conditions
  - When `$\mu$ = 1`, even though the null hypothesis is wrong, the
    power of testing is still low.  
  - With the true mean increases, the power of testing increases
    sharply.

## are the estimations unbiased or not

``` r
mu_values <- 0:6

df_mu_hat <- 

  map_dfr(
    mu_values,
    function(true_mu){
      sim <- simulate_tt(true_mu)
      avg_mu_hat <- mean(sim$mu_hat)
      avg_reject <- mean(sim$mu_hat[sim$p_value < 0.05], na.rm = TRUE)
      
      tibble(
        true_mu ,
        avg_mu_hat,
        avg_reject
      )
    }
  )

df_mu_hat |>
  ggplot(aes(x = true_mu, y = avg_mu_hat)) +
  geom_line(color = "#5a6") + geom_point(size = 2, color = "red")
```

![](p8105_hw5_tw3128_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

``` r
df_mu_hat |>
  ggplot(aes(x = true_mu, y = avg_reject)) +
  geom_abline(intercept = 0, slope = 1, color = "pink") +
  geom_line(color = "#5a6") + geom_point(size = 2, color = "red") +
  labs(
    title = expression("Averege " * hat(mu) * " vs Real Expectations When Significant")
  )
```

![](p8105_hw5_tw3128_files/figure-gfm/unnamed-chunk-8-2.png)<!-- -->

- No, the average $\mu_(hat)$ is not equal to the real expectation when
  significant. Instead, it is systematically larger than the true
  mean.  
- This happens because we are conditioning on statistical significance
  (p \< 0.05). When μ is small, only samples with unusually large
  positive estimation errors are likely to produce a significant result
  and lead to rejection of the null. As a result, the subset of
  significant samples is biased.

# Problem Three

``` r
df_homicide <- read_csv("data/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

## create “city_state” and describe the dataset

``` r
df_homicide <-
  df_homicide |>
  mutate(city_state = paste(city, state, sep = ", "))

df_city_summary <-
  df_homicide |>
  group_by(city_state) |>
  summarise(
    total_homicide = n(),
    unsolved_homicide = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

df_city_summary
```

    ## # A tibble: 51 × 3
    ##    city_state      total_homicide unsolved_homicide
    ##    <chr>                    <int>             <int>
    ##  1 Albuquerque, NM            378               146
    ##  2 Atlanta, GA                973               373
    ##  3 Baltimore, MD             2827              1825
    ##  4 Baton Rouge, LA            424               196
    ##  5 Birmingham, AL             800               347
    ##  6 Boston, MA                 614               310
    ##  7 Buffalo, NY                521               319
    ##  8 Charlotte, NC              687               206
    ##  9 Chicago, IL               5535              4073
    ## 10 Cincinnati, OH             694               309
    ## # ℹ 41 more rows

The homicide dataset consists of individual-level case records compiled
by the Washington Post. Each row represents a single homicide. The
dataset includes:  
\* Demographic Data: name, age, sex, and race  
\* Location: city, state, latitude and longitude  
\* Time: reported_date  
\* Case disposition: whether the case was “Closed by arrest”, “Closed
without arrest”, or remains “Open/No arrest”  
In total, the dataset contains 52179 observations and 12 variables.

## Baltimore

``` r
df_btm <-
  df_city_summary |>
  filter(city_state == "Baltimore, MD") 

btm_total <- df_btm$total_homicide
btm_unsolved <- df_btm$unsolved_homicide

df_tt_btm <- prop.test(btm_unsolved, btm_total) |>
  broom::tidy() |>
  select(estimate, conf.low, conf.high) |>
  janitor::clean_names()
```

## all cities

``` r
df_tt_all <-
  df_city_summary |>
  mutate(
    test = map2(unsolved_homicide, total_homicide, ~ prop.test(.x, .y)),
    tidy_test = map(test, ~ tidy(.x))
  ) |>
  unnest(tidy_test) |>
  select(city_state, estimate, conf.low, conf.high) |>
  janitor::clean_names()
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `test = map2(unsolved_homicide, total_homicide, ~prop.test(.x,
    ##   .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

## plotting

``` r
df_plot <-
  df_tt_all |>
  arrange(estimate) |>
  mutate(city_state = factor(city_state, levels = city_state))

ggplot(df_plot, aes(x = city_state, y = estimate)) +
  geom_point(color = "red") +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high), width = 0.2) +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Unsolved Proportion"
  ) +
   theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
  )
```

![](p8105_hw5_tw3128_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->
